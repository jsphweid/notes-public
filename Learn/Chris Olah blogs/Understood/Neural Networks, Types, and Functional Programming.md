# Neural Networks, Types, and Functional Programming (2015)

Deep Learning is a young field and in an ad-hoc state. No formalism, no real understanding, just the wild west.

layers of NN is like function composition. What we're doing in training is optimizing the functions.

types → embedding data in *n* bits

representations → embedding a data manifold in *n* dimensions

A lot of NN structures could be viewed as a library of FP functions. Programming using FP means gluing these functions together to perform tasks. Arguably this is similar to NN because you can glue together parts of networks in order to accomplish a task.

Chris believes there are three competing narratives:

1. neuroscience narrative, how Artificial NNs can be compared to Biological ones
2. representations narrative, centered on representations and manifolds
3. probabilistic narrative, centered on NNs finding latent variables