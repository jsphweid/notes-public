# Deep Learning, NLP, and Representations (2014)

[https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)

word embeddings are a very easy-to-understand product of a NN optimization process. They are representations that can be inspected and manipulated with clarity even by us humans (male - female + queen = king)

often word embeddings are created for a certain ML task and used in another ML task. This often goes by multiples names:

- pre-training
- transfer learning
- multi-task learning

We can use it also do map multiple kinds of data into *one* representation

The importance of representations is the main point of this article it seems. 

`why are neural networks effective? Because better ways of representing data can pop out of optimizing layered models.`