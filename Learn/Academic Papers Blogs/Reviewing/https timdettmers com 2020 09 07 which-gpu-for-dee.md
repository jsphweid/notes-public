# https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/

tensor cores are fast, but the biggest constraints is moving around memory, so memory bandwidth is important

fp16 - floating point 16, relatively small range

bf16 - brian float 16, 16 bits but uses more bits for exponent, there’s a bit of precision loss but there’s a huge range