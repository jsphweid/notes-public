# Attention

Attention in the context of NNs is giving curated visibility to other inputs (self-attention) or outputs from different layers (decoder looking at encoded data?)

Helpful links:

[https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)

[https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)