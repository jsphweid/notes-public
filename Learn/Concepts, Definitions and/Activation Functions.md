# Activation Functions

decide whether or not — or maybe more specifically 'how much' —  a neuron 'fires' 

for learning (backpropagation) derivatives are important here because it allows us to mathematically adjust the weights in the correct direction (higher or lower) to optimize the result next time