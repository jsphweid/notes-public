# Cross-Entropy

Way of calculating loss between two probability distributions in ML

Commonly you'll have a one-hot encoding as 1 distribution and often a NN output (softmax) as the other distribution. Cross-Entropy determines the loss from these two.

![[/Untitled.png]]